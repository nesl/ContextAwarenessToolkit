# ContextAwarenessToolkit
A suite of apps to assist developers in different stages of building context inferences apps:
- [Data Collector](https://github.com/nesl/ContextAwarenessToolkit/tree/master/apps/DataCollector) for collecting sensor data and ground truth labels.
- [Inference Composer](https://github.com/nesl/ContextAwarenessToolkit/tree/master/apps/InferenceComposer) that automatically composes and exports an inference pipeline from a collected dataset, including the training and tuning of machine learning models
- [Inference Executor](https://github.com/nesl/ContextAwarenessToolkit/tree/master/apps/InferenceExecutor) that takes an exported inference pipeline from Inference Composer and performs runtime optimizations of inference executions by partitioning a pipeline across multiple devices. 

## Data Collector
This is an Android app for helping developers collect a set of sensor data from users, including the ground truth label. 

### Usage
The basic workflow of Data Collector is described as follows:
- Data Collector provides a set of APIs for app developers to specify the set of sensors to be considered, including device types, sensor types, sensor configurations, as well as ground truth configurations.
- Target users then install Data Collector, which collects data based on the developer’s configuration. The app runs on the target user’s phone, continuously sampling sensor data and periodically querying ground truth labels (if necessary) until the desired amount of data has been collected.
- Finally, Data Collector outputs the data in a serialized or structured human-readable format, and developers can save the data for later use.

### Implementation
Data Collector is implemented as an Android app. It provides a `DataCollectionConfigurator` for developers to configure a data collection and then calls a background service `DataCollectionService` for sampling and saving sensor data as a `DataVector`. The ground truth label collection is implemented using the Android Alarm and BroadcastReceiver and periodically queries labels from users.

### API Example
```
// Configure type of sensor data to collect
DataCollectionConfigurator configurator =
    new DataCollectionConfigurator();
configurator.addSensorTypeToVector(
    DeviceType.ANDROID_PHONE,
    SensorType.ANDROID_ACCELEROMETER);
configurator.addSensorTypeToVector(
    DeviceType.ANDROID_PHONE,
    SensorType.ANDROID_GRAVITY);

// Configure label collection
LabelType labelType = new LabelType(
    "ground_truth",
    LabelDataType.NOMINAL);
    labelType.setInterval(10);
    labelType.addCandidateNominalValue("Activity A");
    labelType.addCandidateNominalValue("Activity B");
    labelType.addCandidateNominalValue("Activity C");

// Pass Configurator to DataCollectionService
DataCollectionService.configureDataCollection(
    DeviceType.ANDROID_PHONE,
    configurator.getDataVector());

// Start the data collection
DataCollectionService.startDataCollection();

// Stop the data collection and obtain the data as CSV
DataVector result = DataCollectionService.stopDataCollection();
result.dumpAsCSV("path_to_data.csv");
```
The above example uses Data Collector’s APIs to configure a data collection. Developers invoke the configurator and specify the types of sensor data to be collected. They then request labels of nominal type (strings) to be collected and specify the possible candidate strings. Finally, developers use the Data Collection Service to start the data collection, and write the collected data to a CSV file after stopping the collection.

## Inference Composer
Inference Composer is a Python program that provides high-level APIs for training machine learning models from data generated by Data Collector. It eases the inference model training by offering an automatic training mode, where a set of default classifiers are trained on a dataset and the model with the best performance is returned to the developer.
Moveover, Inference Composer closes the gap between popular machine learning frameworks (e.g. scikit-learn and Tensorflow [ten]) and mobile operating systems such as Android. 
Inference Composer can export an inference pipeline trained by these frameworks (in Python) to a JSON file that can be parsed, initialized, and executed on Android using the Inference Executor app described next.
In doing so it saves trained classification models in the form of [Predictive Model Markup Language (PMML)](https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language) which can then be loaded by the Inference Executor.

### Usage
With Inference Composer, **machine learning experts** (i.e. inference developers) create inference modules by implementing a `ModuleBase` interface. Each inference module must implement two methods:
- `process()` takes a data vector, performs inference computation, and returns a data vector.
- `export()` saves the module itself in JSON format, including transforming scikit classifiers to PMML using [sklearn2pmml](https://github.com/jpmml/sklearn2pmml).

**App developers** then feed sensor data into Inference Composer. Inference Composer offer two modes of composing inferences:
- Manual mode: App developers use the API to specify the sensors to be considered, pre-processing functions, feature functions, and classification models (given that these modules exist in the current module library). By supplying a dataset obtained from Data Collector, an inference pipeline can be trained based on the specifications and saved to a JSON file.
- Automatic mode: App developers use the API to pick a specific inference and specify cer- tain criteria, for example, a thresholding inference accuracy or simply requesting a classifier with the best accuracy. Based on the dataset, Inference Composer automatically selects the right classifier and feature algorithms (if required) from the current module library to meet the criteria, and returns the trained inference pipeline.

After an inference pipeline is created, it can be exported to a JSON file by invoking the `export()` method of each of its member inference module. 

### Implementation
Inference Composer is implemented in Python and provides wrapper functions for both scikit-learn and Tensorflow, enabling it to leverage both traditional machine learning models and deep neural networks. It provides a set of default inference modules but also allows inference developers to add new modules using the ModuleBase interface. In order to export scikit classifiers into PMML, Inference Composer uses the open-source [sklearn2pmml].

### API Example
```
# Pre-processing
pre_processor = Preprocess.Preprocess(
  _window_size=1,
  _data_columns=data_columns,
  _operators=[Preprocess.MOVING_AVG_SMOOTH]
processed_data = pre_processor.process(raw_data)

# Feature calculation
feature_calculator = Feature.Feature(
  _window_size=1,
  _data_columns=data_columns,
  _features=features
)
feature_vector = feature_calculator.process(processed_data)

# Train default classifiers and show performance
classifiers = Classifier.Classifier(
   _feature_mapper=feature_calculator.get_mapper(),
    _model_path=MODEL_PATH,
    _cross_validation=True,
    _cv_fold=10
)
classifiers.add_default_classifiers()
classifiers.process(feature_vector)

# Export the trained pipeline to JSON,
# including automatically transforming model to PMML
export_inference([pre_processor, feature_calculator, classifiers])
```
Developers configure pre-processing, feature calculation, and classifiers using existing inference modules. They can export the entire inference as a JSON file after specifying each modules.



